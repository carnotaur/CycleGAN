{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import glob\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, path_A, path_B):\n",
    "        self.to_tensor = transforms.Compose([\n",
    "                                transforms.Resize((128, 128)), \n",
    "                                transforms.ToTensor()])\n",
    "        self.re_number = re.compile('[0-9]')\n",
    "        self.files_A = self._get_files_path(path_A)\n",
    "        self.files_B = self._get_files_path(path_B)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files_A)\n",
    "    \n",
    "    def _file_numbers(self, x):\n",
    "        filename = x.split('/')[-1]\n",
    "        file_number = int(''.join(self.re_number.findall(filename)))\n",
    "        return file_number\n",
    "    \n",
    "    def _get_files_path(self, path):\n",
    "        path_files = glob.glob(path)\n",
    "        path_files = sorted(path_files, key=self._file_numbers)\n",
    "        return path_files\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        A = self.to_tensor(Image.open(self.files_A[idx]))\n",
    "        B = self.to_tensor(Image.open(self.files_B[idx]))\n",
    "        return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_A = 'maps/trainA/*'\n",
    "train_path_B = 'maps/trainB/*'\n",
    "val_path_A = 'maps/valA/*'\n",
    "val_path_B = 'maps/valB/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_path_A, train_path_B)\n",
    "val_dataset = Dataset(val_path_A, val_path_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_genererated_X = 'fake_X'\n",
    "path_genererated_Y = 'fake_Y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Additional Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x_fake, y_fake, epoch, batch_idx):\n",
    "    torchvision.utils.save_image(x_fake.data, f'{path_genererated_X}/{epoch}_{batch_idx}.png', \n",
    "                                 normalize=True)\n",
    "    torchvision.utils.save_image(y_fake.data, f'{path_genererated_Y}/{epoch}_{batch_idx}.png', \n",
    "                                 normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Generator Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DKLayer, self).__init__()\n",
    "        self.refl_padding = nn.ReflectionPad2d(1)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=0)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = self.refl_padding(batch)\n",
    "        batch = self.conv(batch)\n",
    "        batch = self.instance_norm(batch)\n",
    "        batch = F.relu(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.refl_padding_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_channels, kernel_size=3, padding=0)\n",
    "        self.refl_padding_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv2 = nn.Conv2d(n_channels, n_channels, kernel_size=3, padding=0)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        padded_batch = self.refl_padding_1(batch)\n",
    "        conv_batch = self.conv1(padded_batch)\n",
    "        padded_batch = self.refl_padding_2(conv_batch)\n",
    "        conv_batch = self.conv2(padded_batch)\n",
    "        out = batch + conv_batch\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_res=6):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_res = num_res\n",
    "        #make layers\n",
    "        self.refl_padding_1 = nn.ReflectionPad2d(3)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=0)\n",
    "        self.instance_norm1 = nn.InstanceNorm2d(32)\n",
    "        self.dk_layer_1 = DKLayer(32, 64)\n",
    "        self.dk_layer_2 = DKLayer(64, 128)\n",
    "        self.res_blocks = nn.ModuleList([ResidualBlock(128) for _ in range(num_res)])\n",
    "        \n",
    "        self.conv_trans_1 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, \n",
    "                                               output_padding=1)\n",
    "        self.instance_norm2 = nn.InstanceNorm2d(64)\n",
    "        self.conv_trans_2 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, \n",
    "                                               output_padding=1)\n",
    "        self.instance_norm3 = nn.InstanceNorm2d(32)\n",
    "        \n",
    "        self.refl_padding_2 = nn.ReflectionPad2d(3)\n",
    "        self.conv2 = nn.Conv2d(32, 3, kernel_size=7, stride=1, padding=0)\n",
    "        #init weights\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        #print('IN', batch.shape)\n",
    "        batch = self.refl_padding_1(batch)\n",
    "        batch = self.conv1(batch)\n",
    "        batch = self.instance_norm1(batch)\n",
    "        batch = F.relu(batch)\n",
    "        #print('R', batch.shape)\n",
    "        batch = self.dk_layer_1(batch)\n",
    "        #print('DK1', batch.shape)\n",
    "        batch = self.dk_layer_2(batch)\n",
    "        #print('DK2', batch.shape)\n",
    "        #res blocks\n",
    "        for i in range(self.num_res):\n",
    "            batch = self.res_blocks[i](batch)\n",
    "        #print('RES', batch.shape)\n",
    "        #deconvolutions\n",
    "        batch = self.conv_trans_1(batch)\n",
    "        batch = self.instance_norm2(batch)\n",
    "        batch = F.relu(batch)\n",
    "        #print('DEC1', batch.shape)\n",
    "        batch = self.conv_trans_2(batch)\n",
    "        batch = self.instance_norm3(batch)\n",
    "        batch = F.relu(batch)\n",
    "        #print('DEC2', batch.shape)\n",
    "        batch = self.refl_padding_2(batch)\n",
    "        batch = self.conv2(batch)\n",
    "        batch = torch.tanh(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Discriminator Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=2):\n",
    "        super(ConvolutionNorm, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=4, padding=1)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "    def forward(self, batch, use_norm=True):\n",
    "        batch = self.conv(batch)\n",
    "        if use_norm:\n",
    "            batch = self.instance_norm(batch)\n",
    "        batch = F.leaky_relu(batch, 0.2)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = ConvolutionNorm(3, 64)\n",
    "        self.conv2 = ConvolutionNorm(64, 128)\n",
    "        self.conv3 = ConvolutionNorm(128, 256)\n",
    "        self.conv4 = ConvolutionNorm(256, 512, stride=1)\n",
    "        self.conv_out = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = self.conv1(batch, use_norm=False)\n",
    "        batch = self.conv2(batch)\n",
    "        batch = self.conv3(batch)\n",
    "        outputs = self.conv4(batch)\n",
    "        logits = self.conv_out(outputs)\n",
    "        #logits = logits.squeeze(3).squeeze(2)\n",
    "        probs = torch.sigmoid(logits)#.squeeze()\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def cycle_loss(generator_x, generator_y, x, y):\\n    fake_x = generator_x(y)\\n    fake_y = generator_y(x)\\n    proxy_y  = generator_y(fake_x)\\n    proxy_x = generator_x(fake_y)\\n    loss = (proxy_y - y).mean() + (proxy_x - x).mean()\\n    return loss'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def cycle_loss(generator_x, generator_y, x, y):\n",
    "    fake_x = generator_x(y)\n",
    "    fake_y = generator_y(x)\n",
    "    proxy_y  = generator_y(fake_x)\n",
    "    proxy_x = generator_x(fake_y)\n",
    "    loss = (proxy_y - y).mean() + (proxy_x - x).mean()\n",
    "    return loss'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_loss = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(discriminator, real_input, fake_input):\n",
    "    prob_real = discriminator(real_input)\n",
    "    real = torch.ones_like(prob_real).to(prob_real.device)\n",
    "    prob_fake = discriminator(fake_input)\n",
    "    fake = torch.zeros_like(prob_fake).to(prob_fake.device)\n",
    "    \n",
    "    real_loss = mse_criterion(prob_real, real)\n",
    "    fake_loss = mse_criterion(prob_fake, fake)\n",
    "    loss = (real_loss + fake_loss) / 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(discriminator, fake_input):\n",
    "    prob_fake = discriminator(fake_input)\n",
    "    real = torch.ones_like(prob_fake).to(prob_fake.device)\n",
    "    loss = mse_criterion(prob_fake, real)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step(D_x, D_y, G_x, G_y, x, y, pass_steps=50, lambda_coef=10):\n",
    "    #Generators\n",
    "    for _ in range(pass_steps):\n",
    "        fake_x = G_y(y)\n",
    "        fake_y = G_x(x)\n",
    "        rec_x = G_y(fake_y)\n",
    "        rec_y = G_x(fake_x)\n",
    "        gen_loss_x = generator_loss(D_y, fake_x)\n",
    "        gen_loss_y = generator_loss(D_x, fake_y)\n",
    "        #cycle loss\n",
    "        c_loss_x = cycle_loss(rec_x, x)\n",
    "        c_loss_y = cycle_loss(rec_y, y)\n",
    "        gen_loss = gen_loss_x + gen_loss_y + lambda_coef * (c_loss_x + c_loss_y)\n",
    "        #optimization\n",
    "        optimizer_G.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        optimizer_G.step()\n",
    "    #Discriminators\n",
    "    dis_y = discriminator_loss(D_y, y, fake_x.detach())\n",
    "    dis_x = discriminator_loss(D_x, x, fake_y.detach())\n",
    "    #step\n",
    "    optimizer_D.zero_grad()\n",
    "    dis_x.backward()\n",
    "    dis_y.backward()\n",
    "    optimizer_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 6\n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_x = Discriminator().to(device)\n",
    "D_y = Discriminator().to(device)\n",
    "G_x = Generator().to(device)\n",
    "G_y = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(itertools.chain(G_x.parameters(), G_y.parameters()),\n",
    "                               lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(D_x.parameters(), D_y.parameters()),\n",
    "                               lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, Y_batch in val_dataloader:\n",
    "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulat/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: pic saved\n",
      "  100: pic saved\n",
      "  0: pic saved\n",
      "  100: pic saved\n",
      "  0: pic saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fb285864f98>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 494, in Client\n",
      "    deliver_challenge(c, authkey)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 722, in deliver_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100: pic saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fb270170b70>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 155, in recvfds\n",
      "    raise EOFError\n",
      "EOFError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: pic saved\n",
      "  100: pic saved\n",
      "  0: pic saved\n",
      "  100: pic saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fb270170898>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: pic saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fb27117def0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 494, in Client\n",
      "    deliver_challenge(c, authkey)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 722, in deliver_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/dulat/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(num_epochs):\n",
    "    batch_idx = 0\n",
    "    for X_batch, Y_batch in train_dataloader:\n",
    "        #optimize\n",
    "        one_step(D_x, D_y, G_x, G_y, X_batch.to(device), Y_batch.to(device), pass_steps=5)\n",
    "        #plot fake, real\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'  {batch_idx}: pic saved')\n",
    "            with torch.no_grad():\n",
    "                for X_batch, Y_batch in val_dataloader:\n",
    "                    plot(G_x(Y_batch.to(device)), G_y(X_batch.to(device)), epoch, batch_idx)\n",
    "                    break\n",
    "        batch_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
