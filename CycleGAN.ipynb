{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, path_A, path_B, img_size=(128, 128)):\n",
    "        self.to_tensor = transforms.Compose([\n",
    "                                transforms.Resize(img_size), \n",
    "                                transforms.ToTensor()])\n",
    "        self.re_number = re.compile('[0-9]')\n",
    "        self.files_A = self._get_files_path(path_A)\n",
    "        self.files_B = self._get_files_path(path_B)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files_A)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        A = self.to_tensor(Image.open(self.files_A[idx]))\n",
    "        B = self.to_tensor(Image.open(self.files_B[idx]))\n",
    "        return A, B\n",
    "    \n",
    "    def _file_numbers(self, x):\n",
    "        filename = x.stem\n",
    "        file_number = int(''.join(self.re_number.findall(filename)))\n",
    "        return file_number\n",
    "    \n",
    "    def _get_files_path(self, directory):\n",
    "        path_files = directory.glob('*')\n",
    "        path_files = sorted(path_files, key=self._file_numbers)\n",
    "        return path_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_A = Path('data/monet2photo/trainA')\n",
    "train_path_B = Path('data/monet2photo/trainB')\n",
    "val_path_A = Path('data/monet2photo/testA')\n",
    "val_path_B = Path('data/monet2photo/testB')\n",
    "dir_generated_X = Path('data/monet2photo/fake_X')\n",
    "dir_generated_Y = Path('data/monet2photo/fake_Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_path_A, train_path_B)\n",
    "val_dataset = Dataset(val_path_A, val_path_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pics(fake, real, \n",
    "              epoch: int, batch_idx: int, \n",
    "              dir_generated):\n",
    "    path_fake = dir_generated.joinpath(f'fake_{epoch}_{batch_idx}.png')\n",
    "    path_real = dir_generated.joinpath(f'real_{epoch}_{batch_idx}.png')\n",
    "    # saving\n",
    "    torchvision.utils.save_image(fake.data, \n",
    "                                 path_fake, \n",
    "                                 normalize=True)\n",
    "    torchvision.utils.save_image(real.data, \n",
    "                                 path_real, \n",
    "                                 normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_generations(dataset, G_x, G_y, dir_generated_X, dir_generated_Y):\n",
    "    # get random index \n",
    "    ind = np.random.randint(len(dataset))\n",
    "    # generate\n",
    "    with torch.no_grad():\n",
    "        G_x.eval(), G_y.eval()\n",
    "        x_real, y_real = dataset[ind]\n",
    "        x_real, y_real = x_real.unsqueeze(0), y_real.unsqueeze(0)\n",
    "        y_fake = G_x(x_real.to(device)).cpu()\n",
    "        x_fake = G_y(y_real.to(device)).cpu()\n",
    "    \n",
    "    # saving pictures\n",
    "    save_pics(y_fake, x_real,\n",
    "              epoch=epoch, batch_idx=batch_idx, \n",
    "              dir_generated=dir_generated_X)\n",
    "    save_pics(x_fake, y_real,\n",
    "              epoch=epoch, batch_idx=batch_idx, \n",
    "              dir_generated=dir_generated_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Generator Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DKLayer, self).__init__()\n",
    "        self.refl_padding = nn.ReflectionPad2d(1)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=0)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = self.refl_padding(batch)\n",
    "        batch = self.conv(batch)\n",
    "        batch = self.instance_norm(batch)\n",
    "        batch = F.relu(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.refl_padding_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_channels, kernel_size=3, padding=0)\n",
    "        self.refl_padding_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv2 = nn.Conv2d(n_channels, n_channels, kernel_size=3, padding=0)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        padded_batch = self.refl_padding_1(batch)\n",
    "        conv_batch = self.conv1(padded_batch)\n",
    "        padded_batch = self.refl_padding_2(conv_batch)\n",
    "        conv_batch = self.conv2(padded_batch)\n",
    "        out = batch + conv_batch\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_res=6):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_res = num_res\n",
    "        #make layers\n",
    "        self.refl_padding_1 = nn.ReflectionPad2d(3)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=0)\n",
    "        self.instance_norm1 = nn.InstanceNorm2d(32)\n",
    "        self.dk_layer_1 = DKLayer(32, 64)\n",
    "        self.dk_layer_2 = DKLayer(64, 128)\n",
    "        self.res_blocks = nn.ModuleList([ResidualBlock(128) for _ in range(num_res)])\n",
    "        \n",
    "        self.conv_trans_1 = nn.ConvTranspose2d(128, 64, kernel_size=3, \n",
    "                                               stride=2, padding=1, \n",
    "                                               output_padding=1)\n",
    "        self.instance_norm2 = nn.InstanceNorm2d(64)\n",
    "        self.conv_trans_2 = nn.ConvTranspose2d(64, 32, kernel_size=3, \n",
    "                                               stride=2, padding=1, \n",
    "                                               output_padding=1)\n",
    "        self.instance_norm3 = nn.InstanceNorm2d(32)\n",
    "        \n",
    "        self.refl_padding_2 = nn.ReflectionPad2d(3)\n",
    "        self.conv2 = nn.Conv2d(32, 3, kernel_size=7, stride=1, padding=0)\n",
    "        #init weights\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        #print('IN', batch.shape)\n",
    "        batch = self.refl_padding_1(batch)\n",
    "        batch = self.conv1(batch)\n",
    "        batch = self.instance_norm1(batch)\n",
    "        batch = F.relu(batch)\n",
    "        #print('R', batch.shape)\n",
    "        batch = self.dk_layer_1(batch)\n",
    "        #print('DK1', batch.shape)\n",
    "        batch = self.dk_layer_2(batch)\n",
    "        #print('DK2', batch.shape)\n",
    "        #res blocks\n",
    "        for i in range(self.num_res):\n",
    "            batch = self.res_blocks[i](batch)\n",
    "        #print('RES', batch.shape)\n",
    "        #deconvolutions\n",
    "        batch = self.conv_trans_1(batch)\n",
    "        batch = self.instance_norm2(batch)\n",
    "        batch = F.relu(batch)\n",
    "        #print('DEC1', batch.shape)\n",
    "        batch = self.conv_trans_2(batch)\n",
    "        batch = self.instance_norm3(batch)\n",
    "        batch = F.relu(batch)\n",
    "        #print('DEC2', batch.shape)\n",
    "        batch = self.refl_padding_2(batch)\n",
    "        batch = self.conv2(batch)\n",
    "        batch = torch.tanh(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Discriminator Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=2):\n",
    "        super(ConvolutionNorm, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, stride=stride, \n",
    "                              kernel_size=4, padding=1)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, batch, use_norm=True):\n",
    "        batch = self.conv(batch)\n",
    "        if use_norm:\n",
    "            batch = self.instance_norm(batch)\n",
    "        batch = F.leaky_relu(batch, 0.2)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = ConvolutionNorm(3, 64)\n",
    "        self.conv2 = ConvolutionNorm(64, 128)\n",
    "        self.conv3 = ConvolutionNorm(128, 256)\n",
    "        self.conv4 = ConvolutionNorm(256, 512, stride=1)\n",
    "        self.conv_out = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = self.conv1(batch, use_norm=False)\n",
    "        batch = self.conv2(batch)\n",
    "        batch = self.conv3(batch)\n",
    "        outputs = self.conv4(batch)\n",
    "        logits = self.conv_out(outputs)\n",
    "        #logits = logits.squeeze(3).squeeze(2)\n",
    "        probs = torch.sigmoid(logits)#.squeeze()\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cycle_loss(generator_x, generator_y, x, y):\n",
    "#     fake_x = generator_x(y)\n",
    "#     fake_y = generator_y(x)\n",
    "#     proxy_y  = generator_y(fake_x)\n",
    "#     proxy_x = generator_x(fake_y)\n",
    "#     loss = (proxy_y - y).mean() + (proxy_x - x).mean()\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_loss = torch.nn.L1Loss()\n",
    "mse_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(discriminator, real_input, fake_input):\n",
    "    prob_real = discriminator(real_input)\n",
    "    real = torch.ones_like(prob_real).to(prob_real.device)\n",
    "    prob_fake = discriminator(fake_input)\n",
    "    fake = torch.zeros_like(prob_fake).to(prob_fake.device)\n",
    "    \n",
    "    real_loss = mse_criterion(prob_real, real)\n",
    "    fake_loss = mse_criterion(prob_fake, fake)\n",
    "    loss = (real_loss + fake_loss) / 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_gan_loss(discriminator, fake_input):\n",
    "    prob_fake = discriminator(fake_input)\n",
    "    real = torch.ones_like(prob_fake).to(prob_fake.device)\n",
    "    loss = mse_criterion(prob_fake, real)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_total_loss(x, y,\n",
    "                         fake_x, fake_y, \n",
    "                         rec_x, rec_y, \n",
    "                         D_x, D_y, lambda_coef: float = 10):\n",
    "    # GAN losses\n",
    "    gen_loss_x = generator_gan_loss(D_y, fake_x)\n",
    "    gen_loss_y = generator_gan_loss(D_x, fake_y)\n",
    "    # Cycle loss\n",
    "    c_loss_x = cycle_loss(rec_x, x)\n",
    "    c_loss_y = cycle_loss(rec_y, y)\n",
    "    # total loss\n",
    "    loss = gen_loss_x + gen_loss_y + lambda_coef * (c_loss_x + c_loss_y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step(D_x, D_y, G_x, G_y, x, y, pass_steps=50, lambda_coef=10):\n",
    "    G_x.train(), G_y.train()\n",
    "    # Generators steps\n",
    "    for _ in range(pass_steps):\n",
    "        fake_x = G_y(y)\n",
    "        fake_y = G_x(x)\n",
    "        rec_x = G_y(fake_y)\n",
    "        rec_y = G_x(fake_x)\n",
    "        # compute generator loss\n",
    "        gen_loss = generator_total_loss(x, y,\n",
    "                                        fake_x, fake_y, \n",
    "                                        rec_x, rec_y,\n",
    "                                        D_x, D_y, \n",
    "                                        lambda_coef=lambda_coef)\n",
    "        optimizer_G.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "    # Discriminators step\n",
    "    dis_y = discriminator_loss(D_y, y, fake_x.detach())\n",
    "    dis_x = discriminator_loss(D_x, x, fake_y.detach())\n",
    "    optimizer_D.zero_grad()\n",
    "    dis_x.backward()\n",
    "    dis_y.backward()\n",
    "    optimizer_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 1\n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, num_workers=6)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_x = Discriminator().to(device)\n",
    "D_y = Discriminator().to(device)\n",
    "G_x = Generator().to(device)\n",
    "G_y = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(itertools.chain(G_x.parameters(), G_y.parameters()),\n",
    "                               lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(D_x.parameters(), D_y.parameters()),\n",
    "                               lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, \n",
    "                                                lr_lambda=lambda epoch: 0.99 ** epoch)\n",
    "scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, \n",
    "                                                lr_lambda=lambda epoch: 0.99 ** epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, Y_batch in val_dataloader:\n",
    "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 17\n",
    "models_dir = Path('models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51_0: pic saved\n",
      "51_100: pic saved\n",
      "51_200: pic saved\n",
      "51_300: pic saved\n",
      "51_400: pic saved\n",
      "51_500: pic saved\n",
      "51_600: pic saved\n",
      "51_700: pic saved\n",
      "51_800: pic saved\n",
      "51_900: pic saved\n",
      "51_1000: pic saved\n",
      "52_0: pic saved\n",
      "52_100: pic saved\n",
      "52_200: pic saved\n",
      "52_300: pic saved\n",
      "52_400: pic saved\n",
      "52_500: pic saved\n",
      "52_600: pic saved\n",
      "52_700: pic saved\n",
      "52_800: pic saved\n",
      "52_900: pic saved\n",
      "52_1000: pic saved\n",
      "53_0: pic saved\n",
      "53_100: pic saved\n",
      "53_200: pic saved\n",
      "53_300: pic saved\n",
      "53_400: pic saved\n",
      "53_500: pic saved\n",
      "53_600: pic saved\n",
      "53_700: pic saved\n",
      "53_800: pic saved\n",
      "53_900: pic saved\n",
      "53_1000: pic saved\n",
      "54_0: pic saved\n",
      "54_100: pic saved\n",
      "54_200: pic saved\n",
      "54_300: pic saved\n",
      "54_400: pic saved\n",
      "54_500: pic saved\n",
      "54_600: pic saved\n",
      "54_700: pic saved\n",
      "54_800: pic saved\n",
      "54_900: pic saved\n",
      "54_1000: pic saved\n",
      "55_0: pic saved\n",
      "55_100: pic saved\n",
      "55_200: pic saved\n",
      "55_300: pic saved\n",
      "55_400: pic saved\n",
      "55_500: pic saved\n",
      "55_600: pic saved\n",
      "55_700: pic saved\n",
      "55_800: pic saved\n",
      "55_900: pic saved\n",
      "55_1000: pic saved\n",
      "56_0: pic saved\n",
      "56_100: pic saved\n",
      "56_200: pic saved\n",
      "56_300: pic saved\n",
      "56_400: pic saved\n",
      "56_500: pic saved\n",
      "56_600: pic saved\n",
      "56_700: pic saved\n",
      "56_800: pic saved\n",
      "56_900: pic saved\n",
      "56_1000: pic saved\n",
      "57_0: pic saved\n",
      "57_100: pic saved\n",
      "57_200: pic saved\n",
      "57_300: pic saved\n",
      "57_400: pic saved\n",
      "57_500: pic saved\n",
      "57_600: pic saved\n",
      "57_700: pic saved\n",
      "57_800: pic saved\n",
      "57_900: pic saved\n",
      "57_1000: pic saved\n",
      "58_0: pic saved\n",
      "58_100: pic saved\n",
      "58_200: pic saved\n",
      "58_300: pic saved\n",
      "58_400: pic saved\n",
      "58_500: pic saved\n",
      "58_600: pic saved\n",
      "58_700: pic saved\n",
      "58_800: pic saved\n",
      "58_900: pic saved\n",
      "58_1000: pic saved\n",
      "59_0: pic saved\n",
      "59_100: pic saved\n",
      "59_200: pic saved\n",
      "59_300: pic saved\n",
      "59_400: pic saved\n",
      "59_500: pic saved\n",
      "59_600: pic saved\n",
      "59_700: pic saved\n",
      "59_800: pic saved\n",
      "59_900: pic saved\n",
      "59_1000: pic saved\n",
      "60_0: pic saved\n",
      "60_100: pic saved\n",
      "60_200: pic saved\n",
      "60_300: pic saved\n",
      "60_400: pic saved\n",
      "60_500: pic saved\n",
      "60_600: pic saved\n",
      "60_700: pic saved\n",
      "60_800: pic saved\n",
      "60_900: pic saved\n",
      "60_1000: pic saved\n",
      "61_0: pic saved\n",
      "61_100: pic saved\n",
      "61_200: pic saved\n",
      "61_300: pic saved\n",
      "61_400: pic saved\n",
      "61_500: pic saved\n",
      "61_600: pic saved\n",
      "61_700: pic saved\n",
      "61_800: pic saved\n",
      "61_900: pic saved\n",
      "61_1000: pic saved\n",
      "62_0: pic saved\n",
      "62_100: pic saved\n",
      "62_200: pic saved\n",
      "62_300: pic saved\n",
      "62_400: pic saved\n",
      "62_500: pic saved\n",
      "62_600: pic saved\n",
      "62_700: pic saved\n",
      "62_800: pic saved\n",
      "62_900: pic saved\n",
      "62_1000: pic saved\n",
      "63_0: pic saved\n",
      "63_100: pic saved\n",
      "63_200: pic saved\n",
      "63_300: pic saved\n",
      "63_400: pic saved\n",
      "63_500: pic saved\n",
      "63_600: pic saved\n",
      "63_700: pic saved\n",
      "63_800: pic saved\n",
      "63_900: pic saved\n",
      "63_1000: pic saved\n",
      "64_0: pic saved\n",
      "64_100: pic saved\n",
      "64_200: pic saved\n",
      "64_300: pic saved\n",
      "64_400: pic saved\n",
      "64_500: pic saved\n",
      "64_600: pic saved\n",
      "64_700: pic saved\n",
      "64_800: pic saved\n",
      "64_900: pic saved\n",
      "64_1000: pic saved\n",
      "65_0: pic saved\n",
      "65_100: pic saved\n",
      "65_200: pic saved\n",
      "65_300: pic saved\n",
      "65_400: pic saved\n",
      "65_500: pic saved\n",
      "65_600: pic saved\n",
      "65_700: pic saved\n",
      "65_800: pic saved\n",
      "65_900: pic saved\n",
      "65_1000: pic saved\n",
      "66_0: pic saved\n",
      "66_100: pic saved\n",
      "66_200: pic saved\n",
      "66_300: pic saved\n",
      "66_400: pic saved\n",
      "66_500: pic saved\n",
      "66_600: pic saved\n",
      "66_700: pic saved\n",
      "66_800: pic saved\n",
      "66_900: pic saved\n",
      "66_1000: pic saved\n",
      "67_0: pic saved\n",
      "67_100: pic saved\n",
      "67_200: pic saved\n",
      "67_300: pic saved\n",
      "67_400: pic saved\n",
      "67_500: pic saved\n",
      "67_600: pic saved\n",
      "67_700: pic saved\n",
      "67_800: pic saved\n",
      "67_900: pic saved\n",
      "67_1000: pic saved\n",
      "68_0: pic saved\n",
      "68_100: pic saved\n",
      "68_200: pic saved\n",
      "68_300: pic saved\n",
      "68_400: pic saved\n",
      "68_500: pic saved\n",
      "68_600: pic saved\n",
      "68_700: pic saved\n",
      "68_800: pic saved\n",
      "68_900: pic saved\n",
      "68_1000: pic saved\n",
      "69_0: pic saved\n",
      "69_100: pic saved\n",
      "69_200: pic saved\n",
      "69_300: pic saved\n",
      "69_400: pic saved\n",
      "69_500: pic saved\n",
      "69_600: pic saved\n",
      "69_700: pic saved\n",
      "69_800: pic saved\n",
      "69_900: pic saved\n",
      "69_1000: pic saved\n",
      "70_0: pic saved\n",
      "70_100: pic saved\n",
      "70_200: pic saved\n",
      "70_300: pic saved\n",
      "70_400: pic saved\n",
      "70_500: pic saved\n",
      "70_600: pic saved\n",
      "70_700: pic saved\n",
      "70_800: pic saved\n",
      "70_900: pic saved\n",
      "70_1000: pic saved\n",
      "71_0: pic saved\n",
      "71_100: pic saved\n",
      "71_200: pic saved\n",
      "71_300: pic saved\n",
      "71_400: pic saved\n",
      "71_500: pic saved\n",
      "71_600: pic saved\n",
      "71_700: pic saved\n",
      "71_800: pic saved\n",
      "71_900: pic saved\n",
      "71_1000: pic saved\n",
      "72_0: pic saved\n",
      "72_100: pic saved\n",
      "72_200: pic saved\n",
      "72_300: pic saved\n",
      "72_400: pic saved\n",
      "72_500: pic saved\n",
      "72_600: pic saved\n",
      "72_700: pic saved\n",
      "72_800: pic saved\n",
      "72_900: pic saved\n",
      "72_1000: pic saved\n",
      "73_0: pic saved\n",
      "73_100: pic saved\n",
      "73_200: pic saved\n",
      "73_300: pic saved\n",
      "73_400: pic saved\n",
      "73_500: pic saved\n",
      "73_600: pic saved\n",
      "73_700: pic saved\n",
      "73_800: pic saved\n",
      "73_900: pic saved\n",
      "73_1000: pic saved\n",
      "74_0: pic saved\n",
      "74_100: pic saved\n",
      "74_200: pic saved\n",
      "74_300: pic saved\n",
      "74_400: pic saved\n",
      "74_500: pic saved\n",
      "74_600: pic saved\n",
      "74_700: pic saved\n",
      "74_800: pic saved\n",
      "74_900: pic saved\n",
      "74_1000: pic saved\n",
      "75_0: pic saved\n",
      "75_100: pic saved\n",
      "75_200: pic saved\n",
      "75_300: pic saved\n",
      "75_400: pic saved\n",
      "75_500: pic saved\n",
      "75_600: pic saved\n",
      "75_700: pic saved\n",
      "75_800: pic saved\n",
      "75_900: pic saved\n",
      "75_1000: pic saved\n",
      "76_0: pic saved\n",
      "76_100: pic saved\n",
      "76_200: pic saved\n",
      "76_300: pic saved\n",
      "76_400: pic saved\n",
      "76_500: pic saved\n",
      "76_600: pic saved\n",
      "76_700: pic saved\n",
      "76_800: pic saved\n",
      "76_900: pic saved\n",
      "76_1000: pic saved\n",
      "77_0: pic saved\n",
      "77_100: pic saved\n",
      "77_200: pic saved\n",
      "77_300: pic saved\n",
      "77_400: pic saved\n",
      "77_500: pic saved\n",
      "77_600: pic saved\n",
      "77_700: pic saved\n",
      "77_800: pic saved\n",
      "77_900: pic saved\n",
      "77_1000: pic saved\n",
      "78_0: pic saved\n",
      "78_100: pic saved\n",
      "78_200: pic saved\n",
      "78_300: pic saved\n",
      "78_400: pic saved\n",
      "78_500: pic saved\n",
      "78_600: pic saved\n",
      "78_700: pic saved\n",
      "78_800: pic saved\n",
      "78_900: pic saved\n",
      "78_1000: pic saved\n",
      "79_0: pic saved\n",
      "79_100: pic saved\n",
      "79_200: pic saved\n",
      "79_300: pic saved\n",
      "79_400: pic saved\n",
      "79_500: pic saved\n",
      "79_600: pic saved\n",
      "79_700: pic saved\n",
      "79_800: pic saved\n",
      "79_900: pic saved\n",
      "79_1000: pic saved\n",
      "80_0: pic saved\n",
      "80_100: pic saved\n",
      "80_200: pic saved\n",
      "80_300: pic saved\n",
      "80_400: pic saved\n",
      "80_500: pic saved\n",
      "80_600: pic saved\n",
      "80_700: pic saved\n",
      "80_800: pic saved\n",
      "80_900: pic saved\n",
      "80_1000: pic saved\n",
      "81_0: pic saved\n",
      "81_100: pic saved\n",
      "81_200: pic saved\n",
      "81_300: pic saved\n",
      "81_400: pic saved\n",
      "81_500: pic saved\n",
      "81_600: pic saved\n",
      "81_700: pic saved\n",
      "81_800: pic saved\n",
      "81_900: pic saved\n",
      "81_1000: pic saved\n",
      "82_0: pic saved\n",
      "82_100: pic saved\n",
      "82_200: pic saved\n",
      "82_300: pic saved\n",
      "82_400: pic saved\n",
      "82_500: pic saved\n",
      "82_600: pic saved\n",
      "82_700: pic saved\n",
      "82_800: pic saved\n",
      "82_900: pic saved\n",
      "82_1000: pic saved\n",
      "83_0: pic saved\n",
      "83_100: pic saved\n",
      "83_200: pic saved\n",
      "83_300: pic saved\n",
      "83_400: pic saved\n",
      "83_500: pic saved\n",
      "83_600: pic saved\n",
      "83_700: pic saved\n",
      "83_800: pic saved\n",
      "83_900: pic saved\n",
      "83_1000: pic saved\n",
      "84_0: pic saved\n",
      "84_100: pic saved\n",
      "84_200: pic saved\n",
      "84_300: pic saved\n",
      "84_400: pic saved\n",
      "84_500: pic saved\n",
      "84_600: pic saved\n",
      "84_700: pic saved\n",
      "84_800: pic saved\n",
      "84_900: pic saved\n",
      "84_1000: pic saved\n",
      "85_0: pic saved\n",
      "85_100: pic saved\n",
      "85_200: pic saved\n",
      "85_300: pic saved\n",
      "85_400: pic saved\n",
      "85_500: pic saved\n",
      "85_600: pic saved\n",
      "85_700: pic saved\n",
      "85_800: pic saved\n",
      "85_900: pic saved\n",
      "85_1000: pic saved\n",
      "86_0: pic saved\n",
      "86_100: pic saved\n",
      "86_200: pic saved\n",
      "86_300: pic saved\n",
      "86_400: pic saved\n",
      "86_500: pic saved\n",
      "86_600: pic saved\n",
      "86_700: pic saved\n",
      "86_800: pic saved\n",
      "86_900: pic saved\n",
      "86_1000: pic saved\n",
      "87_0: pic saved\n",
      "87_100: pic saved\n",
      "87_200: pic saved\n",
      "87_300: pic saved\n",
      "87_400: pic saved\n",
      "87_500: pic saved\n",
      "87_600: pic saved\n",
      "87_700: pic saved\n",
      "87_800: pic saved\n",
      "87_900: pic saved\n",
      "87_1000: pic saved\n",
      "88_0: pic saved\n",
      "88_100: pic saved\n",
      "88_200: pic saved\n",
      "88_300: pic saved\n",
      "88_400: pic saved\n",
      "88_500: pic saved\n",
      "88_600: pic saved\n",
      "88_700: pic saved\n",
      "88_800: pic saved\n",
      "88_900: pic saved\n",
      "88_1000: pic saved\n",
      "89_0: pic saved\n",
      "89_100: pic saved\n",
      "89_200: pic saved\n",
      "89_300: pic saved\n",
      "89_400: pic saved\n",
      "89_500: pic saved\n",
      "89_600: pic saved\n",
      "89_700: pic saved\n",
      "89_800: pic saved\n",
      "89_900: pic saved\n",
      "89_1000: pic saved\n",
      "90_0: pic saved\n",
      "90_100: pic saved\n",
      "90_200: pic saved\n",
      "90_300: pic saved\n",
      "90_400: pic saved\n",
      "90_500: pic saved\n",
      "90_600: pic saved\n",
      "90_700: pic saved\n",
      "90_800: pic saved\n",
      "90_900: pic saved\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-ca3739dfc2e4>\u001b[0m in \u001b[0;36mone_step\u001b[0;34m(D_x, D_y, G_x, G_y, x, y, pass_steps, lambda_coef)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                         \u001b[0mrec_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                         \u001b[0mD_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                         lambda_coef=lambda_coef)\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mgen_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-d4350ceff91f>\u001b[0m in \u001b[0;36mgenerator_total_loss\u001b[0;34m(x, y, fake_x, fake_y, rec_x, rec_y, D_x, D_y, lambda_coef)\u001b[0m\n\u001b[1;32m      4\u001b[0m                          D_x, D_y, lambda_coef: float = 10):\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# GAN losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mgen_loss_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_gan_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mgen_loss_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_gan_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Cycle loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-8ab4012b2d57>\u001b[0m in \u001b[0;36mgenerator_gan_loss\u001b[0;34m(discriminator, fake_input)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprob_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mreal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2155\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2156\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while epoch <= num_epochs:\n",
    "    for batch_idx, (X_batch, Y_batch) in enumerate(train_dataloader):\n",
    "        # optimize\n",
    "        one_step(D_x, D_y, G_x, G_y, \n",
    "                 X_batch.to(device), \n",
    "                 Y_batch.to(device), pass_steps=5)\n",
    "        # save fake, real\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'{epoch}_{batch_idx}: pic saved')\n",
    "            watch_generations(val_dataset, G_x, G_y, \n",
    "                              dir_generated_X, dir_generated_Y)\n",
    "    \n",
    "    # saving model\n",
    "    torch.save(G_x, models_dir.joinpath(f'GX_{epoch}'))\n",
    "    torch.save(G_y, models_dir.joinpath(f'GY_{epoch}'))\n",
    "    torch.save(D_x, models_dir.joinpath(f'DX_{epoch}'))\n",
    "    torch.save(D_y, models_dir.joinpath(f'DY_{epoch}'))\n",
    "    \n",
    "    #make step\n",
    "    if epoch >= 100:\n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "        \n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAIAAABMXPacAAAARElEQVR4nO3BAQEAAACAkP6v7ggKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAYwIAAAWMWdQAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=128x128 at 0x7FA2C1317978>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open('data/monet2photo/fake_X/fake_90_800.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
