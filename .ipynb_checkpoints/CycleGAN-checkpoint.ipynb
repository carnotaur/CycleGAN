{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, path_A, path_B, img_size=(128, 128)):\n",
    "        self.to_tensor = transforms.Compose([\n",
    "                                transforms.Resize(img_size), \n",
    "                                transforms.ToTensor()])\n",
    "        self.re_number = re.compile('[0-9]')\n",
    "        self.files_A = self._get_files_path(path_A)\n",
    "        self.files_B = self._get_files_path(path_B)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files_A)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        A = self.to_tensor(Image.open(self.files_A[idx]))\n",
    "        B = self.to_tensor(Image.open(self.files_B[idx]))\n",
    "        return A, B\n",
    "    \n",
    "    def _file_numbers(self, x):\n",
    "        filename = x.stem\n",
    "        file_number = int(''.join(self.re_number.findall(filename)))\n",
    "        return file_number\n",
    "    \n",
    "    def _get_files_path(self, directory):\n",
    "        path_files = directory.glob('*')\n",
    "        path_files = sorted(path_files, key=self._file_numbers)\n",
    "        return path_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_A = Path('data/monet2photo/trainA')\n",
    "train_path_B = Path('data/monet2photo/trainB')\n",
    "val_path_A = Path('data/monet2photo/testA')\n",
    "val_path_B = Path('data/monet2photo/testB')\n",
    "dir_generated_X = Path('data/monet2photo/fake_X')\n",
    "dir_generated_Y = Path('data/monet2photo/fake_Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_path_A, train_path_B)\n",
    "val_dataset = Dataset(val_path_A, val_path_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pics(fake, real, \n",
    "              epoch: int, batch_idx: int, \n",
    "              dir_generated):\n",
    "    path_fake = dir_generated.joinpath(f'fake_{epoch}_{batch_idx}.png')\n",
    "    path_real = dir_generated.joinpath(f'real_{epoch}_{batch_idx}.png')\n",
    "    # saving\n",
    "    torchvision.utils.save_image(fake.data, \n",
    "                                 path_fake, \n",
    "                                 normalize=True)\n",
    "    torchvision.utils.save_image(real.data, \n",
    "                                 path_real, \n",
    "                                 normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_generations(dataset, G_x, G_y, dir_generated_X, dir_generated_Y):\n",
    "    # get random index \n",
    "    ind = np.random.randint(len(dataset))\n",
    "    # generate\n",
    "    with torch.no_grad():\n",
    "        G_x.eval(), G_y.eval()\n",
    "        x_real, y_real = dataset[ind]\n",
    "        x_real, y_real = x_real.unsqueeze(0), y_real.unsqueeze(0)\n",
    "        y_fake = G_x(x_real.to(device)).cpu()\n",
    "        x_fake = G_y(y_real.to(device)).cpu()\n",
    "    \n",
    "    # saving pictures\n",
    "    save_pics(y_fake, x_real,\n",
    "              epoch=epoch, batch_idx=batch_idx, \n",
    "              dir_generated=dir_generated_X)\n",
    "    save_pics(x_fake, y_real,\n",
    "              epoch=epoch, batch_idx=batch_idx, \n",
    "              dir_generated=dir_generated_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Generator Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DKLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DKLayer, self).__init__()\n",
    "        self.refl_padding = nn.ReflectionPad2d(1)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=0)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = self.refl_padding(batch)\n",
    "        batch = self.conv(batch)\n",
    "        batch = self.instance_norm(batch)\n",
    "        batch = F.relu(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, n_channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.refl_padding_1 = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_channels, kernel_size=3, padding=0)\n",
    "        self.refl_padding_2 = nn.ReflectionPad2d(1)\n",
    "        self.conv2 = nn.Conv2d(n_channels, n_channels, kernel_size=3, padding=0)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        padded_batch = self.refl_padding_1(batch)\n",
    "        conv_batch = self.conv1(padded_batch)\n",
    "        padded_batch = self.refl_padding_2(conv_batch)\n",
    "        conv_batch = self.conv2(padded_batch)\n",
    "        out = batch + conv_batch\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_res=6):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_res = num_res\n",
    "        #make layers\n",
    "        self.refl_padding_1 = nn.ReflectionPad2d(3)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7, stride=1, padding=0)\n",
    "        self.instance_norm1 = nn.InstanceNorm2d(32)\n",
    "        self.dk_layer_1 = DKLayer(32, 64)\n",
    "        self.dk_layer_2 = DKLayer(64, 128)\n",
    "        self.res_blocks = nn.ModuleList([ResidualBlock(128) for _ in range(num_res)])\n",
    "        \n",
    "        self.conv_trans_1 = nn.ConvTranspose2d(128, 64, kernel_size=3, \n",
    "                                               stride=2, padding=1, \n",
    "                                               output_padding=1)\n",
    "        self.instance_norm2 = nn.InstanceNorm2d(64)\n",
    "        self.conv_trans_2 = nn.ConvTranspose2d(64, 32, kernel_size=3, \n",
    "                                               stride=2, padding=1, \n",
    "                                               output_padding=1)\n",
    "        self.instance_norm3 = nn.InstanceNorm2d(32)\n",
    "        \n",
    "        self.refl_padding_2 = nn.ReflectionPad2d(3)\n",
    "        self.conv2 = nn.Conv2d(32, 3, kernel_size=7, stride=1, padding=0)\n",
    "        #init weights\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        #print('IN', batch.shape)\n",
    "        batch = self.refl_padding_1(batch)\n",
    "        batch = self.conv1(batch)\n",
    "        batch = self.instance_norm1(batch)\n",
    "        batch = F.relu(batch)\n",
    "        #print('R', batch.shape)\n",
    "        batch = self.dk_layer_1(batch)\n",
    "        #print('DK1', batch.shape)\n",
    "        batch = self.dk_layer_2(batch)\n",
    "        #print('DK2', batch.shape)\n",
    "        #res blocks\n",
    "        for i in range(self.num_res):\n",
    "            batch = self.res_blocks[i](batch)\n",
    "        #print('RES', batch.shape)\n",
    "        #deconvolutions\n",
    "        batch = self.conv_trans_1(batch)\n",
    "        batch = self.instance_norm2(batch)\n",
    "        batch = F.relu(batch)\n",
    "        #print('DEC1', batch.shape)\n",
    "        batch = self.conv_trans_2(batch)\n",
    "        batch = self.instance_norm3(batch)\n",
    "        batch = F.relu(batch)\n",
    "        #print('DEC2', batch.shape)\n",
    "        batch = self.refl_padding_2(batch)\n",
    "        batch = self.conv2(batch)\n",
    "        batch = torch.tanh(batch)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Discriminator Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=2):\n",
    "        super(ConvolutionNorm, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, stride=stride, \n",
    "                              kernel_size=4, padding=1)\n",
    "        self.instance_norm = nn.InstanceNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, batch, use_norm=True):\n",
    "        batch = self.conv(batch)\n",
    "        if use_norm:\n",
    "            batch = self.instance_norm(batch)\n",
    "        batch = F.leaky_relu(batch, 0.2)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = ConvolutionNorm(3, 64)\n",
    "        self.conv2 = ConvolutionNorm(64, 128)\n",
    "        self.conv3 = ConvolutionNorm(128, 256)\n",
    "        self.conv4 = ConvolutionNorm(256, 512, stride=1)\n",
    "        self.conv_out = nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n",
    "        self.apply(weights_init)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch = self.conv1(batch, use_norm=False)\n",
    "        batch = self.conv2(batch)\n",
    "        batch = self.conv3(batch)\n",
    "        outputs = self.conv4(batch)\n",
    "        logits = self.conv_out(outputs)\n",
    "        #logits = logits.squeeze(3).squeeze(2)\n",
    "        probs = torch.sigmoid(logits)#.squeeze()\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def cycle_loss(generator_x, generator_y, x, y):\n",
    "#     fake_x = generator_x(y)\n",
    "#     fake_y = generator_y(x)\n",
    "#     proxy_y  = generator_y(fake_x)\n",
    "#     proxy_x = generator_x(fake_y)\n",
    "#     loss = (proxy_y - y).mean() + (proxy_x - x).mean()\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_loss = torch.nn.L1Loss()\n",
    "mse_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(discriminator, real_input, fake_input):\n",
    "    prob_real = discriminator(real_input)\n",
    "    real = torch.ones_like(prob_real).to(prob_real.device)\n",
    "    prob_fake = discriminator(fake_input)\n",
    "    fake = torch.zeros_like(prob_fake).to(prob_fake.device)\n",
    "    \n",
    "    real_loss = mse_criterion(prob_real, real)\n",
    "    fake_loss = mse_criterion(prob_fake, fake)\n",
    "    loss = (real_loss + fake_loss) / 2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_gan_loss(discriminator, fake_input):\n",
    "    prob_fake = discriminator(fake_input)\n",
    "    real = torch.ones_like(prob_fake).to(prob_fake.device)\n",
    "    loss = mse_criterion(prob_fake, real)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_total_loss(x, y,\n",
    "                         fake_x, fake_y, \n",
    "                         rec_x, rec_y, \n",
    "                         D_x, D_y, lambda_coef: float = 10):\n",
    "    # GAN losses\n",
    "    gen_loss_x = generator_gan_loss(D_y, fake_x)\n",
    "    gen_loss_y = generator_gan_loss(D_x, fake_y)\n",
    "    # Cycle loss\n",
    "    c_loss_x = cycle_loss(rec_x, x)\n",
    "    c_loss_y = cycle_loss(rec_y, y)\n",
    "    # total loss\n",
    "    loss = gen_loss_x + gen_loss_y + lambda_coef * (c_loss_x + c_loss_y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step(D_x, D_y, G_x, G_y, x, y, pass_steps=50, lambda_coef=10):\n",
    "    G_x.train(), G_y.train()\n",
    "    # Generators steps\n",
    "    for _ in range(pass_steps):\n",
    "        fake_x = G_y(y)\n",
    "        fake_y = G_x(x)\n",
    "        rec_x = G_y(fake_y)\n",
    "        rec_y = G_x(fake_x)\n",
    "        # compute generator loss\n",
    "        gen_loss = generator_total_loss(x, y,\n",
    "                                        fake_x, fake_y, \n",
    "                                        rec_x, rec_y,\n",
    "                                        D_x, D_y, \n",
    "                                        lambda_coef=lambda_coef)\n",
    "        optimizer_G.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "    # Discriminators step\n",
    "    dis_y = discriminator_loss(D_y, y, fake_x.detach())\n",
    "    dis_x = discriminator_loss(D_x, x, fake_y.detach())\n",
    "    optimizer_D.zero_grad()\n",
    "    dis_x.backward()\n",
    "    dis_y.backward()\n",
    "    optimizer_D.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 1\n",
    "lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=True, num_workers=6)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_x = Discriminator().to(device)\n",
    "D_y = Discriminator().to(device)\n",
    "G_x = Generator().to(device)\n",
    "G_y = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(itertools.chain(G_x.parameters(), G_y.parameters()),\n",
    "                               lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D = torch.optim.Adam(itertools.chain(D_x.parameters(), D_y.parameters()),\n",
    "                               lr=lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, \n",
    "                                                lr_lambda=lambda epoch: 0.99 ** epoch)\n",
    "scheduler_D = torch.optim.lr_scheduler.LambdaLR(optimizer_D, \n",
    "                                                lr_lambda=lambda epoch: 0.99 ** epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, Y_batch in val_dataloader:\n",
    "    X_batch, Y_batch = X_batch.to(device), Y_batch.to(device) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 17\n",
    "models_dir = Path('models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "while epoch <= num_epochs:\n",
    "    for batch_idx, (X_batch, Y_batch) in enumerate(train_dataloader):\n",
    "        # optimize\n",
    "        one_step(D_x, D_y, G_x, G_y, \n",
    "                 X_batch.to(device), \n",
    "                 Y_batch.to(device), pass_steps=5)\n",
    "        # save fake, real\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'{epoch}_{batch_idx}: pic saved')\n",
    "            watch_generations(val_dataset, G_x, G_y, \n",
    "                              dir_generated_X, dir_generated_Y)\n",
    "    \n",
    "    # saving model\n",
    "    torch.save(G_x, models_dir.joinpath(f'GX_{epoch}'))\n",
    "    torch.save(G_y, models_dir.joinpath(f'GY_{epoch}'))\n",
    "    torch.save(D_x, models_dir.joinpath(f'DX_{epoch}'))\n",
    "    torch.save(D_y, models_dir.joinpath(f'DY_{epoch}'))\n",
    "    \n",
    "    #make step\n",
    "    if epoch >= 100:\n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "        \n",
    "    epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
